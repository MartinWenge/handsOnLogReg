{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Session 1 Machine Learning: Logistic Regression\n",
    "\n",
    "In the next block, you find all the commands used in the hands on session as a lookup.\n",
    "To get out most of the session, try to type everything by yourself and understand what you are typing and what happens.\n",
    "\n",
    "Tipps:\n",
    "\n",
    "* use somehow small execution blocks and print some example output to see what happens\n",
    "* add a **lot of comments** to remember what you did\n",
    "* in python loops, function definitions, and so on need the correct indenting\n",
    "* in jupyter notebooks, the cells are executed with 'shift + enter'\n",
    "* you can add markdown for formatted comments by toggling the cell type in the menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### setup the data:\n",
    "# np.random.seed(12)\n",
    "# num_observations = 5000\n",
    "# x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "# x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "# simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "# simulated_labels = np.hstack((np.zeros(num_observations), np.ones(num_observations)))\n",
    "\n",
    "###### make a plot without any additional output:\n",
    "# figure()\n",
    "# scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1], c = simulated_labels, alpha = .4)\n",
    "# plot([-2,3],[0,4])\n",
    "# show()\n",
    "\n",
    "###### define important functions:\n",
    "# def sigmoid(scores):\n",
    "#    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# def log_likelihood(features, target, weights):\n",
    "#    scores = np.dot(features, weights)\n",
    "#    return np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "\n",
    "# def grad_log_likelyhood(features, weights, target):\n",
    "#     output_error_signal = target - sigmoid(np.dot(features, weights))\n",
    "#     return np.dot(features.T, output_error_signal)\n",
    "\n",
    "# def logistic_regression(features, target, num_steps, learning_rate):\n",
    "#    intercept = np.ones((features.shape[0], 1))\n",
    "#    features = np.hstack((intercept, features))\n",
    "#    weights = np.zeros(features.shape[1])\n",
    "#    for step in range(num_steps):\n",
    "#        gradient = grad_log_likelyhood(features, weights, target)\n",
    "#        weights += learning_rate * gradient\n",
    "#        if step % 10000 == 0:\n",
    "#            print(log_likelihood(features, target, weights))\n",
    "#    return weights\n",
    "\n",
    "###### run the model\n",
    "# num_steps = 300000\n",
    "# learning_rate = 5e-5\n",
    "# weights = logistic_regression(simulated_separableish_features, simulated_labels, num_steps, learning_rate)\n",
    "# print(weights)\n",
    "\n",
    "###### compare with scikit learn\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# clf = LogisticRegression(fit_intercept=True, C = 1e15) # C = 1e15 basically turns of an optimzation to be comparable to our function\n",
    "# clf.fit(simulated_separableish_features, simulated_labels)\n",
    "# print(clf.intercept_, clf.coef_)\n",
    "# print(weights)\n",
    "\n",
    "###### get the accuracy\n",
    "# data_with_intercept = np.hstack((np.ones((simulated_separableish_features.shape[0], 1)), simulated_separableish_features))\n",
    "# final_scores = np.dot(data_with_intercept, weights)\n",
    "# preds = np.round(sigmoid(final_scores))\n",
    "# print('Accuracy from scratch: {}'.format((preds == simulated_labels).sum().astype(float) / len(preds)) )\n",
    "# print('Accuracy from sk-learn: {}'.format(clf.score(simulated_separableish_features, simulated_labels)))\n",
    "\n",
    "# figure()\n",
    "# scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1], c = preds == simulated_labels - 1, alpha = .8)#, s = 30)\n",
    "# show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we go with your own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# get access to numpy and matplotlib\n",
    "%pylab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
